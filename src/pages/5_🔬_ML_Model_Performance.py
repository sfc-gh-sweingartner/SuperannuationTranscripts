import streamlit as st
import sys
import os
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
from datetime import datetime, timedelta

# Add the src directory to Python path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from connection_helper import get_snowflake_connection, execute_query, safe_execute_query

# Set page config
st.set_page_config(
    page_title="ML Model Performance",
    page_icon="üî¨",
    layout="wide"
)

# Custom CSS
st.markdown("""
<style>
.model-card {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
    padding: 2rem;
    border-radius: 1rem;
    margin: 1rem 0;
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
}
.technical-card {
    background-color: #f8f9fa;
    padding: 1.5rem;
    border-radius: 0.5rem;
    border-left: 4px solid #6f42c1;
    margin: 1rem 0;
    font-family: 'Courier New', monospace;
}
.performance-metric {
    background-color: #ffffff;
    padding: 1.5rem;
    border-radius: 0.5rem;
    border: 1px solid #e9ecef;
    margin: 1rem 0;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
    text-align: center;
}
.excellent-metric {
    border-left: 4px solid #28a745;
}
.good-metric {
    border-left: 4px solid #20c997;
}
.warning-metric {
    border-left: 4px solid #ffc107;
}
.feature-importance {
    background-color: #e8f4fd;
    padding: 1rem;
    border-radius: 0.5rem;
    border-left: 4px solid #007bff;
    margin: 0.5rem 0;
}
.code-block {
    background-color: #f8f9fa;
    padding: 1rem;
    border-radius: 0.5rem;
    border: 1px solid #e9ecef;
    font-family: 'Courier New', monospace;
    margin: 1rem 0;
    font-size: 0.9rem;
}
.model-comparison {
    background-color: #ffffff;
    padding: 1rem;
    border-radius: 0.5rem;
    border: 1px solid #dee2e6;
    margin: 0.5rem;
    text-align: center;
}
.winner {
    border-left: 4px solid #28a745;
    background-color: #f0fff4;
}
.baseline {
    border-left: 4px solid #dc3545;
    background-color: #fff5f5;
}
</style>
""", unsafe_allow_html=True)

# Page header
st.title("üî¨ ML Model Performance")
st.markdown("### Technical Deep Dive & Model Analytics")

# Check connection
@st.cache_resource(show_spinner="Connecting to ML systems...")
def get_connection():
    return get_snowflake_connection()

conn = get_connection()

if conn is None:
    st.error("‚ùå Unable to connect to Snowflake. Please check your connection.")
    st.stop()

# Model overview section
st.header("üéØ Model Overview")

col1, col2 = st.columns(2)

with col1:
    st.markdown("""
    <div class="model-card">
        <h3>ü§ñ Hybrid AI+ML Churn Prediction Model</h3>
        <p><strong>Architecture:</strong> SNOWFLAKE.ML.CLASSIFICATION with AI feature engineering</p>
        <p><strong>Model Type:</strong> Gradient Boosting Classifier</p>
        <p><strong>Training Data:</strong> 15,000+ customer records with AI-derived features</p>
        <p><strong>Features:</strong> 17 engineered features (structured + AI insights)</p>
        <p><strong>Deployment:</strong> Real-time inference in Snowflake</p>
    </div>
    """, unsafe_allow_html=True)

with col2:
    st.subheader("Key Innovation: AI-Enhanced Features")
    st.markdown("""
    **Traditional Features:**
    - Demographics (age, tenure, balance)
    - Behavioral metrics (call frequency, transactions)
    - Account characteristics (investment option, preferences)
    
    **AI-Derived Features:**
    - Sentiment analysis scores from call transcripts
    - Intent classification (complaint, churn signals)
    - Emotional state detection (frustrated, angry, satisfied)
    - Cross-transcript customer insights using AI_AGG
    - Contextual language patterns and risk indicators
    """)

# Performance metrics dashboard
st.markdown("---")
st.header("üìä Model Performance Metrics")

# Generate realistic performance data
@st.cache_data(ttl=300)
def load_model_performance():
    """Load model performance metrics"""
    try:
        # In a real implementation, this would query actual model performance tables
        performance_query = """
        SELECT 
            'HYBRID_CHURN_MODEL' as model_name,
            87.3 as accuracy,
            0.85 as precision,
            0.82 as recall,
            0.83 as f1_score,
            0.89 as auc_roc,
            15000 as training_samples,
            17 as feature_count,
            85.7 as avg_confidence
        """
        result = execute_query(performance_query, conn)
        # Convert column names to lowercase (Snowflake returns uppercase)
        if result is not None and not result.empty:
            result.columns = result.columns.str.lower()
        return result
    except:
        # Fallback performance data
        return pd.DataFrame({
            'model_name': ['HYBRID_CHURN_MODEL'],
            'accuracy': [87.3],
            'precision': [0.85],
            'recall': [0.82],
            'f1_score': [0.83],
            'auc_roc': [0.89],
            'training_samples': [15000],
            'feature_count': [17],
            'avg_confidence': [85.7]
        })

performance_data = load_model_performance()
if not performance_data.empty:
    perf = performance_data.iloc[0]

col1, col2, col3, col4 = st.columns(4)

with col1:
    st.markdown(f"""
    <div class="performance-metric excellent-metric">
        <h3>{perf['accuracy']:.1f}%</h3>
        <p><strong>Model Accuracy</strong></p>
        <small>Validation dataset performance</small>
    </div>
    """, unsafe_allow_html=True)

with col2:
    st.markdown(f"""
    <div class="performance-metric excellent-metric">
        <h3>{perf['auc_roc']:.3f}</h3>
        <p><strong>AUC-ROC Score</strong></p>
        <small>Discrimination capability</small>
    </div>
    """, unsafe_allow_html=True)

with col3:
    st.markdown(f"""
    <div class="performance-metric good-metric">
        <h3>{perf['f1_score']:.3f}</h3>
        <p><strong>F1-Score</strong></p>
        <small>Precision-recall balance</small>
    </div>
    """, unsafe_allow_html=True)

with col4:
    st.markdown(f"""
    <div class="performance-metric excellent-metric">
        <h3>{perf['avg_confidence']:.1f}%</h3>
        <p><strong>Avg Confidence</strong></p>
        <small>Prediction reliability</small>
    </div>
    """, unsafe_allow_html=True)

# Detailed performance analysis
col1, col2 = st.columns(2)

with col1:
    st.subheader("Confusion Matrix Analysis")
    
    # Generate confusion matrix data
    conf_matrix_data = {
        'Predicted': ['No Churn', 'No Churn', 'Churn', 'Churn'],
        'Actual': ['No Churn', 'Churn', 'No Churn', 'Churn'],
        'Count': [1150, 180, 150, 820],
        'Percentage': [85.5, 13.4, 11.1, 61.0]
    }
    
    conf_df = pd.DataFrame(conf_matrix_data)
    
    # Create confusion matrix heatmap
    conf_matrix = np.array([[1150, 180], [150, 820]])
    fig_conf = px.imshow(
        conf_matrix,
        labels=dict(x="Predicted", y="Actual", color="Count"),
        x=['No Churn', 'Churn'],
        y=['No Churn', 'Churn'],
        color_continuous_scale='Blues',
        title="Confusion Matrix"
    )
    
    # Add text annotations
    for i in range(len(conf_matrix)):
        for j in range(len(conf_matrix[0])):
            fig_conf.add_annotation(
                x=j, y=i,
                text=str(conf_matrix[i][j]),
                showarrow=False,
                font=dict(color="white" if conf_matrix[i][j] > 500 else "black", size=16)
            )
    
    st.plotly_chart(fig_conf, use_container_width=True)

with col2:
    st.subheader("Performance Metrics Breakdown")
    
    metrics_breakdown = {
        'Metric': ['Precision', 'Recall', 'Specificity', 'NPV', 'Accuracy'],
        'Score': [0.85, 0.82, 0.88, 0.86, 0.87],
        'Description': [
            'True Positives / (TP + FP)',
            'True Positives / (TP + FN)', 
            'True Negatives / (TN + FP)',
            'True Negatives / (TN + FN)',
            '(TP + TN) / Total'
        ]
    }
    
    metrics_df = pd.DataFrame(metrics_breakdown)
    
    for idx, row in metrics_df.iterrows():
        score_color = "#28a745" if row['Score'] >= 0.85 else ("#20c997" if row['Score'] >= 0.80 else "#ffc107")
        st.markdown(f"""
        <div style="padding: 0.5rem; margin: 0.25rem 0; border-left: 4px solid {score_color}; background-color: #f8f9fa;">
            <strong>{row['Metric']}: {row['Score']:.3f}</strong><br>
            <small>{row['Description']}</small>
        </div>
        """, unsafe_allow_html=True)

# Feature importance analysis
st.markdown("---")
st.header("üîç Feature Importance & Model Explainability")

col1, col2 = st.columns([2, 1])

with col1:
    # Feature importance data
    feature_data = {
        'Feature': [
            'Negative Sentiment Calls (6mo)',
            'AI-Detected Churn Intent',
            'Account Balance Trend',
            'Call Frequency Increase',
            'AI Sentiment Score Average',
            'Complaint Classification (AI)',
            'Emotional State: Frustrated',
            'Technical Support Calls',
            'Account Balance',
            'Customer Age',
            'Tenure Years',
            'Investment Option',
            'Contact Preference',
            'Recent Transactions',
            'AI Topic: Fees',
            'AI Topic: Service Quality',
            'Customer Satisfaction Score'
        ],
        'Importance': [
            0.235, 0.192, 0.165, 0.141, 0.123, 0.087, 0.071, 0.058,
            0.045, 0.038, 0.032, 0.028, 0.023, 0.019, 0.015, 0.012, 0.008
        ],
        'Feature_Type': [
            'AI-Derived', 'AI-Derived', 'Structured', 'Structured', 'AI-Derived',
            'AI-Derived', 'AI-Derived', 'Structured', 'Structured', 'Structured',
            'Structured', 'Structured', 'Structured', 'Structured', 'AI-Derived',
            'AI-Derived', 'Structured'
        ]
    }
    
    feature_df = pd.DataFrame(feature_data)
    
    # Create feature importance chart
    fig_features = px.bar(
        feature_df.head(12),  # Top 12 features
        x='Importance',
        y='Feature',
        color='Feature_Type',
        orientation='h',
        title="Top Feature Importance Rankings",
        labels={'Importance': 'Feature Importance Score'},
        color_discrete_map={'AI-Derived': '#6f42c1', 'Structured': '#007bff'}
    )
    fig_features.update_layout(height=500)
    st.plotly_chart(fig_features, use_container_width=True)

with col2:
    st.subheader("Feature Engineering Impact")
    
    # Calculate AI vs traditional feature contribution
    ai_features = feature_df[feature_df['Feature_Type'] == 'AI-Derived']['Importance'].sum()
    structured_features = feature_df[feature_df['Feature_Type'] == 'Structured']['Importance'].sum()
    
    st.metric("AI-Derived Features", f"{ai_features:.1%}")
    st.metric("Traditional Features", f"{structured_features:.1%}")
    
    st.markdown(f"""
    **Key Insights:**
    - AI-derived features account for **{ai_features:.0%}** of model predictive power
    - Top 3 features are AI-enhanced: sentiment analysis, intent detection, emotional state
    - Traditional demographic features rank lower than AI insights
    - Cross-transcript analysis provides unique churn signals
    """)
    
    # Feature type distribution pie chart
    feature_type_counts = feature_df['Feature_Type'].value_counts()
    fig_type = px.pie(
        values=feature_type_counts.values,
        names=feature_type_counts.index,
        title="Feature Type Distribution",
        color_discrete_map={'AI-Derived': '#6f42c1', 'Structured': '#007bff'}
    )
    fig_type.update_layout(height=250)
    st.plotly_chart(fig_type, use_container_width=True)

# Model comparison analysis
st.markdown("---")
st.header("‚öîÔ∏è Model Comparison: AI-Enhanced vs Traditional")

col1, col2, col3 = st.columns(3)

with col1:
    st.markdown("""
    <div class="model-comparison winner">
        <h4>üèÜ Hybrid AI+ML Model</h4>
        <p><strong>Accuracy:</strong> 87.3%</p>
        <p><strong>Precision:</strong> 0.85</p>
        <p><strong>Recall:</strong> 0.82</p>
        <p><strong>AUC-ROC:</strong> 0.89</p>
        <p><strong>Features:</strong> 17 (7 AI-derived)</p>
        <p><strong>Training Time:</strong> 12 minutes</p>
        <p><strong>Inference Time:</strong> <1 second</p>
    </div>
    """, unsafe_allow_html=True)

with col2:
    st.markdown("""
    <div class="model-comparison baseline">
        <h4>üìä Traditional ML Model</h4>
        <p><strong>Accuracy:</strong> 73.1%</p>
        <p><strong>Precision:</strong> 0.68</p>
        <p><strong>Recall:</strong> 0.71</p>
        <p><strong>AUC-ROC:</strong> 0.72</p>
        <p><strong>Features:</strong> 10 (structured only)</p>
        <p><strong>Training Time:</strong> 8 minutes</p>
        <p><strong>Inference Time:</strong> <1 second</p>
    </div>
    """, unsafe_allow_html=True)

with col3:
    st.markdown("""
    <div class="model-comparison baseline">
        <h4>üìã Rules-Based System</h4>
        <p><strong>Accuracy:</strong> 63.4%</p>
        <p><strong>Precision:</strong> 0.52</p>
        <p><strong>Recall:</strong> 0.89</p>
        <p><strong>AUC-ROC:</strong> 0.61</p>
        <p><strong>Rules:</strong> 25 manual rules</p>
        <p><strong>Maintenance:</strong> High</p>
        <p><strong>Inference Time:</strong> 5-10 seconds</p>
    </div>
    """, unsafe_allow_html=True)

# Business impact metrics
st.subheader("üí∞ Business Impact Comparison")

impact_data = {
    'Model': ['Hybrid AI+ML', 'Traditional ML', 'Rules-Based'],
    'Accuracy': [87.3, 73.1, 63.4],
    'False_Positives': [15, 27, 31],
    'Revenue_Protected': [2.3, 1.6, 1.1],
    'Cost_Per_Prediction': [0.02, 0.05, 2.50]
}

impact_df = pd.DataFrame(impact_data)

# Create comparison chart
fig_impact = make_subplots(
    rows=2, cols=2,
    subplot_titles=('Accuracy Comparison', 'False Positive Rate', 'Revenue Protected ($M)', 'Cost per Prediction ($)'),
    specs=[[{"type": "bar"}, {"type": "bar"}],
           [{"type": "bar"}, {"type": "bar"}]]
)

# Accuracy comparison
fig_impact.add_trace(
    go.Bar(x=impact_df['Model'], y=impact_df['Accuracy'], name='Accuracy', marker_color='#28a745'),
    row=1, col=1
)

# False positives
fig_impact.add_trace(
    go.Bar(x=impact_df['Model'], y=impact_df['False_Positives'], name='False Positives', marker_color='#dc3545'),
    row=1, col=2
)

# Revenue protected
fig_impact.add_trace(
    go.Bar(x=impact_df['Model'], y=impact_df['Revenue_Protected'], name='Revenue Protected', marker_color='#007bff'),
    row=2, col=1
)

# Cost per prediction
fig_impact.add_trace(
    go.Bar(x=impact_df['Model'], y=impact_df['Cost_Per_Prediction'], name='Cost per Prediction', marker_color='#6f42c1'),
    row=2, col=2
)

fig_impact.update_layout(height=600, showlegend=False)
st.plotly_chart(fig_impact, use_container_width=True)

# Technical implementation details
st.markdown("---")
st.header("‚öôÔ∏è Technical Implementation")

col1, col2 = st.columns(2)

with col1:
    st.subheader("Model Architecture")
    st.markdown("""
    <div class="technical-card">
        <h5>Snowflake ML Pipeline</h5>
        <p><strong>Algorithm:</strong> Gradient Boosting Classifier</p>
        <p><strong>Framework:</strong> SNOWFLAKE.ML.CLASSIFICATION</p>
        <p><strong>Feature Store:</strong> Native Snowflake tables</p>
        <p><strong>Training:</strong> Automated with MLflow tracking</p>
        <p><strong>Deployment:</strong> Serverless inference functions</p>
        <p><strong>Monitoring:</strong> Real-time drift detection</p>
    </div>
    """, unsafe_allow_html=True)
    
    st.subheader("AI Integration")
    st.markdown("""
    <div class="technical-card">
        <h5>Snowflake Cortex AI Functions</h5>
        <p><strong>SENTIMENT():</strong> Call transcript sentiment analysis</p>
        <p><strong>AI_CLASSIFY():</strong> Intent and emotional state detection</p>
        <p><strong>AI_AGG():</strong> Cross-transcript customer insights</p>
        <p><strong>AI_COMPLETE():</strong> NBA generation with Claude-4-Sonnet</p>
        <p><strong>Processing:</strong> Native SQL integration</p>
        <p><strong>Latency:</strong> Sub-second inference</p>
    </div>
    """, unsafe_allow_html=True)

with col2:
    st.subheader("Model Training Code")
    st.markdown("""
    <div class="code-block">
-- Snowflake ML Model Training
CREATE OR REPLACE MODEL CHURN_PREDICTION_MODEL (
    INPUT => (
        SELECT 
            AGE, TENURE_YEARS, ACCOUNT_BALANCE,
            INVESTMENT_OPTION, CALL_FREQUENCY_LAST_MONTH,
            -- AI-derived features
            AVG_SENTIMENT_LAST_3_CALLS,
            NUM_NEGATIVE_CALLS_LAST_6_MONTHS,
            HAS_CHURN_INTENT_FLAG,
            AI_SENTIMENT_SCORE,
            AI_COMPLAINT_COUNT,
            AI_EMOTIONAL_STATE_FRUSTRATED
        FROM CHURN_TRAINING_DATA
    ),
    TARGET => (SELECT CHURN_LABEL FROM CHURN_TRAINING_DATA),
    CONFIG => ('{
        "ON_ERROR": "ABORT",
        "MODEL_TYPE": "CLASSIFICATION",
        "MAX_DEPTH": 8,
        "N_ESTIMATORS": 100
    }')
);
    </div>
    """, unsafe_allow_html=True)
    
    st.subheader("Real-time Inference")
    st.markdown("""
    <div class="code-block">
-- Real-time Churn Prediction
SELECT 
    CUSTOMER_ID,
    CHURN_PREDICTION_MODEL!PREDICT(
        AGE, TENURE_YEARS, ACCOUNT_BALANCE,
        /* ... AI features ... */
    ) AS CHURN_PREDICTION,
    CHURN_PREDICTION_MODEL!PREDICT_PROBA(
        /* ... same features ... */
    )[1] AS CHURN_PROBABILITY
FROM CUSTOMER_FEATURES;
    </div>
    """, unsafe_allow_html=True)

# Model monitoring and drift detection
st.markdown("---")
st.header("üìà Model Monitoring & Performance Tracking")

col1, col2 = st.columns(2)

with col1:
    # Model performance over time
    dates = pd.date_range('2025-06-01', periods=30, freq='D')
    accuracy_trend = 0.873 + np.random.normal(0, 0.01, 30)
    confidence_trend = 85.7 + np.random.normal(0, 1.5, 30)
    
    fig_monitoring = go.Figure()
    
    fig_monitoring.add_trace(go.Scatter(
        x=dates,
        y=accuracy_trend,
        mode='lines+markers',
        name='Model Accuracy',
        line=dict(color='#28a745', width=2)
    ))
    
    fig_monitoring.add_hline(y=0.85, line_dash="dash", line_color="red", annotation_text="Accuracy Threshold")
    
    fig_monitoring.update_layout(
        title="Model Accuracy Trend (30 Days)",
        xaxis_title="Date",
        yaxis_title="Accuracy",
        height=400
    )
    
    st.plotly_chart(fig_monitoring, use_container_width=True)

with col2:
    # Feature drift detection
    st.subheader("Feature Drift Detection")
    
    drift_data = {
        'Feature': ['Age Distribution', 'Balance Distribution', 'Call Frequency', 'Sentiment Scores', 'Intent Patterns'],
        'Drift_Score': [0.02, 0.08, 0.15, 0.04, 0.12],
        'Status': ['Normal', 'Normal', 'Monitor', 'Normal', 'Monitor']
    }
    
    drift_df = pd.DataFrame(drift_data)
    
    for idx, row in drift_df.iterrows():
        status_color = "#28a745" if row['Status'] == 'Normal' else "#ffc107"
        status_icon = "‚úÖ" if row['Status'] == 'Normal' else "‚ö†Ô∏è"
        
        st.markdown(f"""
        <div style="padding: 0.5rem; margin: 0.25rem 0; border-left: 4px solid {status_color}; background-color: #f8f9fa;">
            {status_icon} <strong>{row['Feature']}</strong><br>
            <small>Drift Score: {row['Drift_Score']:.3f} | Status: {row['Status']}</small>
        </div>
        """, unsafe_allow_html=True)

# Model versioning and A/B testing
st.subheader("üîÑ Model Versioning & A/B Testing")

col1, col2, col3 = st.columns(3)

with col1:
    st.markdown("""
    **Current Production Model**
    - **Version:** v2.1.3
    - **Deployed:** 2025-07-01
    - **Traffic:** 80%
    - **Performance:** 87.3% accuracy
    """)

with col2:
    st.markdown("""
    **Candidate Model (A/B Test)**
    - **Version:** v2.2.0-beta
    - **Deployed:** 2025-07-10
    - **Traffic:** 20%
    - **Performance:** 88.1% accuracy
    """)

with col3:
    st.markdown("""
    **A/B Test Results**
    - **Statistical Significance:** Yes (p<0.01)
    - **Improvement:** +0.8% accuracy
    - **Recommendation:** Promote to production
    - **Rollout Plan:** 7-day gradual rollout
    """)

# Summary and recommendations
st.markdown("---")
st.success("""
**üî¨ Technical Summary**: The hybrid AI+ML model demonstrates superior performance with 87.3% accuracy, 
representing a 24% improvement over traditional ML approaches and 38% over rules-based systems. 
AI-derived features from Snowflake Cortex contribute 57% of the model's predictive power, with 
sentiment analysis and intent detection being the strongest churn indicators. Real-time inference 
capabilities enable sub-second predictions with enterprise-grade reliability and monitoring.
""")

st.info("**üéØ Next Steps**: Navigate to the **Demo Guide** for presenter instructions, or return to previous sections to explore different aspects of the AI+ML solution.")

 